<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><title>Deep Learning in a nutshell - a tour de force</title><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui" name="viewport"><link href="reveal.js/css/reveal.css" rel="stylesheet"><link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.3.0/css/font-awesome.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
tex2jax: {
  inlineMath: [["\\(", "\\)"]],
  displayMath: [["\\[", "\\]"]],
  ignoreClass: "nostem|nolatexmath"
},
asciimath2jax: {
  delimiters: [["\\$", "\\$"]],
  ignoreClass: "nostem|noasciimath"
},
TeX: { equationNumbers: { autoNumber: "none" } }
});</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.4.0/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script><link href="reveal.js/lib/css/zenburn.css" rel="stylesheet"><script>var link = document.createElement( 'link' );
link.rel = 'stylesheet';
link.type = 'text/css';
link.href = window.location.search.match( /print-pdf/gi ) ? "reveal.js/css/print/pdf.css" : "reveal.js/css/print/paper.css";
document.getElementsByTagName( 'head' )[0].appendChild( link );</script><!--[if lt IE 9]><script src="reveal.js/lib/js/html5shiv.js"></script><![endif]--><link rel="stylesheet" href="custom.css"></head><body><div class="reveal"><div class="slides"><section class="title" data-state="title"><h1>Deep Learning in a nutshell - a tour de force</h1><div class="preamble"><div class="paragraph"><p><a href="https://tu-dresden.de/mn/physik/iktp/das-institut/termine/termine/peter-steinbac-machine-learning">IKTP Institute Seminar</a><br></p></div>
<div class="paragraph"><p><a href="mailto:steinbach@scionics.de">Peter Steinbach</a>, October 11, 2018, Dresden, Germany</p></div></div></section>
<section><section id="preface"><h2>Preface</h2></section><section id="my_employer"><h2>My employer</h2><div class="imageblock" style=""><img src="images/scionics_main_logo.png" alt="scionics main logo" width="80%"></div>
<div class="paragraph"><p><a href="https://scionics.de">Scionics Computer Innovation GmbH</a></p></div>
<aside class="notes"><div class="ulist"><ul><li><p>offer scientific consulting</p></li><li><p>data analysis, large data handling, &#8230;&#8203;</p></li></ul></div></aside></section><section id="our_client_mpi_cbg"><h2>Our client: <a href="https://mpi-cbg.de">MPI CBG</a></h2><div class="imageblock" style=""><img src="images/x600px-MPI-CBG_building_outside_4pl.jpg" alt="x600px MPI CBG building outside 4pl"></div>
<div class="paragraph"><p><a href="https://mpi-cbg.de">Max Planck Institute for Molecular Cell Biology and Genetics</a></p></div>
<aside class="notes"><div class="ulist"><ul><li><p>scientific computing facility</p></li><li><p>my role</p></li></ul></div></aside></section><section id="disclaimer"><h2>Disclaimer</h2><div class="imageblock stretch" style=""><img src="images/bart_simpson_white.png" alt="bart simpson white" height="100%"></div>
<div class="paragraph"><p>These slides are open-source:</p></div>
<div class="paragraph"><p><a href="https://github.com/psteinb/deeplearning-at-iktp2018">github.com/psteinb/deeplearning-at-iktp2018</a></p></div></section></section>
<section><section id="a_detour_trackml"><h2>A Detour: <a href="https://indico.cern.ch/event/587955/contributions/2937510/attachments/1683248/2705359/20180712-msmk-trackml-v3.pdf">TrackML</a></h2><div class="imageblock" style=""><img src="images/chep2018-mkiehn-p3.png" alt="chep2018 mkiehn p3"></div><aside class="notes"><div class="ulist"><ul><li><p>Are there better approaches?</p></li><li><p>How to find them?</p></li><li><p>Do we have to do it ourselves?</p></li><li><p>Use challenge format to look for new solutions</p></li></ul></div></aside></section><section id="trackml_flow"><h2><a href="https://indico.cern.ch/event/587955/contributions/2937510/attachments/1683248/2705359/20180712-msmk-trackml-v3.pdf">TrackML flow</a></h2><div class="imageblock" style=""><img src="images/chep2018-mkiehn-p4.png" alt="chep2018 mkiehn p4"></div></section><section id="trackml_data"><h2><a href="https://indico.cern.ch/event/587955/contributions/2937510/attachments/1683248/2705359/20180712-msmk-trackml-v3.pdf">TrackML data</a></h2><div class="imageblock" style=""><img src="images/chep2018-mkiehn-p7.png" alt="chep2018 mkiehn p7"></div>
<aside class="notes"><div class="ulist"><ul><li><p>goal: find tracks that connect the hits</p></li><li><p>no particle ID</p></li></ul></div></aside></section><section id="trackml_challenge"><h2><a href="https://indico.cern.ch/event/587955/contributions/2937510/attachments/1683248/2705359/20180712-msmk-trackml-v3.pdf">TrackML Challenge</a></h2><div class="imageblock" style=""><img src="images/chep2018-mkiehn-p6.png" alt="chep2018 mkiehn p6"></div></section><section id="final_leaderboard"><h2><a href="https://www.kaggle.com/c/trackml-particle-identification/leaderboard">Final Leaderboard</a></h2><div class="paragraph"><p><span class="image"><img src="images/trackml-accuracy-phase-leaderboard_square.png" alt="trackml accuracy phase leaderboard square"></span></p></div>
<aside class="notes"><div class="paragraph"><p>next up: Which approach won?</p></div></aside></section><section id="which_approach_won" data-background-image="images/pexels-bright-bulb-dark-132340_1024px.jpg" data-background-size="cover"><h2>Which approach won?</h2>
<aside class="notes"><div class="ulist"><ul><li><p>Wait until the end!</p></li></ul></div></aside></section><section id="the_no_free_lunch_theorem"><h2>The No Free Lunch Theorem</h2><div class="admonitionblock note"><table><tr><td class="icon"><i class="fa fa-info-circle" title="Note"></i></td><td class="content"><div class="title">By <a href="http://www.no-free-lunch.org/">David Wolpert</a></div><div class="paragraph"><p>Averaged over all possible data generating distributions, every classification algorithm has the same error
rate when classifying previously unobserved points.</p></div></td></tr></table></div>
<aside class="notes"><div class="ulist"><ul><li><p>Ian Goodfellow: No machine learning algorithm is universally any better than any other.</p></li><li><p>Extra: The most sophisticated algorithm we can conceive of has the same average performance (over
all possible tasks) as merely predicting that every point belongs to the same class.</p></li></ul></div></aside></section><section id="what" data-background-image="images/pexels-photo-sad.jpg" data-background-size="cover"><h2>What?</h2></section><section id="machine_learning_means"><h2>Machine Learning means</h2><div class="admonitionblock note"><table><tr><td class="icon"><i class="fa fa-info-circle" title="Note"></i></td><td class="content"><div class="title">By <a href="https://www.deeplearningbook.org/">Ian Goodfellow</a></div><div class="paragraph"><p>This means that the goal of machine learning research is not to seek a universal learning algorithm or the absolute best learning algorithm. Instead, <strong>our goal is to understand what kinds of distributions are relevant to the “real world”</strong> that an AI agent experiences, and what kinds of machine learning algorithms perform well on data drawn from the kinds of data generating distributions we care about.</p></div></td></tr></table></div>
<aside class="notes"><div class="ulist"><ul><li><p>Show you yet another technique: Deep Learning!</p></li></ul></div></aside></section></section>
<section><section id="deep_learning"><h2>Deep Learning</h2><div class="ulist"><ul><li class="fragment"><p>branch of machine learning</p></li><li class="fragment"><p>learning data representations</p></li><li class="fragment"><p>supervised, semi-supervised or unsupervised learning</p></li><li class="fragment"><p>applications in many fields</p></li></ul></div></section><section id="multi_object_detection_with_yolo_v2"><h2>Multi-Object Detection with <a href="https://arxiv.org/abs/1612.08242">Yolo v2</a></h2><div class="videoblock stretch"><iframe width="100%" height="100%" src="https://www.youtube.com/embed/VOC3huqHrss?rel=0&amp;start=5" frameborder="0" allowfullscreen></iframe></div></section><section id="image_alteration"><h2><a href="https://github.com/junyanz/CycleGAN">Image Alteration</a></h2><div class="imageblock" style=""><img src="images/cycleGAN.jpeg" alt="cycleGAN" width="100%"></div>
<div class="paragraph"><p><a href="https://arxiv.org/pdf/1703.10593.pdf)">arxiv:1703.10593</a></p></div></section><section id="image_restoration"><h2><a href="http://csbdeep.bioimagecomputing.com/index.html">Image Restoration</a></h2><div class="paragraph"><p>Planaria Worm</p></div>
<table class="tableblock frame-none grid-none" style="width:100%"><colgroup><col style="width:50%"><col style="width:50%"></colgroup><tbody><tr><td class="tableblock halign-center valign-top"><div><div class="paragraph"><p><span class="image"><img src="images/denoising_planaria_magma_input.png" alt="denoising planaria magma input" width="100%"></span></p></div></div></td><td class="tableblock halign-center valign-top"><div><div class="paragraph"><p><span class="image"><img src="images/denoising_planaria_magma_network.png" alt="denoising planaria magma network" width="100%"></span></p></div></div></td></tr><tr><td class="tableblock halign-center valign-top"><p class="tableblock"><strong>from Microscope</strong></p></td><td class="tableblock halign-center valign-top"><p class="tableblock"><strong>after DL based denoising</strong></p></td></tr></table></section><section id="learning_from_mistakes"><h2>Learning from Mistakes</h2><div class="videoblock stretch"><iframe width="100%" height="100%" src="https://www.youtube.com/embed/TmPfTpjtdgg?rel=0" frameborder="0" allowfullscreen></iframe></div></section><section id="lets_narrow_a_bit"><h2>Let&#8217;s narrow a bit</h2><div class="paragraph"><p><span class="image"><img src="images/narrow-arrows-box-business-533189.jpg" alt="background"></span></p></div></section></section>
<section id="hands_on_deep_convolutional_neural_networks"><h2>Hands-on: (Deep) Convolutional Neural Networks</h2><div class="paragraph"><p><span class="image"><img src="images/1024px-board-broken-builder-209235.jpg" alt="background"></span></p></div></section>
<section><section id="why_does_it_take_so_long"><h2>Why does it take so long?</h2></section><section id="heavy_lifting_inside_cnns"><h2>Heavy-Lifting inside CNNs</h2><table class="tableblock frame-none grid-none" style="width:100%"><colgroup><col style="width:50%"><col style="width:50%"></colgroup><tbody><tr><td class="tableblock halign-center valign-top"><div><div class="paragraph"><p><span class="image"><img src="images/3D_Convolution_Animation.gif" alt="3D Convolution Animation" width="100%"></span></p></div></div></td><td class="tableblock halign-center valign-top"><div><div class="paragraph"><p><span class="image"><img src="images/Matrix_multiplication_diagram_2.png" alt="Matrix multiplication diagram 2" width="100%"></span></p></div></div></td></tr><tr><td class="tableblock halign-center valign-top"><p class="tableblock"><strong>Convolutions</strong></p></td><td class="tableblock halign-center valign-top"><p class="tableblock"><strong>Matrix Operations</strong></p></td></tr></table></section><section id="a_closer_look"><h2>A closer look</h2><div class="ulist"><ul><li><p>Convolutions<br>
\(y_i = \sum_{n = 0}^{N_k} x_{i+/-n}*k_{i+/-n} \)</p></li><li><p>Matrix Operations<br>
\(AB=Y, y_{ij} = \sum_{k} a_{ik} * b_{kj} \)</p></li><li><p>Common?<br>
<strong>Dot Product Structure!</strong></p></li></ul></div>
<aside class="notes"><div class="ulist"><ul><li><p>thousands of dot-products</p></li><li><p>one HD frame with 3x3 kernel:
 2.067.604 independent pixels
35.149.268 flops
37.216.872 loads
 2.067.604 stores</p></li></ul></div></aside></section><section id="where_do_cpus_come_from"><h2>Where do CPUs come from ?</h2><div class="imageblock" style=""><img src="images/wing-commander.jpg" alt="wing commander" width="100%"></div>
<div class="paragraph"><p>Low Latency Matters Most</p></div>
<aside class="notes"><div class="ulist"><ul><li><p>PC users don&#8217;t want to wait!</p></li></ul></div></aside></section><section id="gpus_for_deep_learning_12"><h2>GPUs for Deep Learning 1/2</h2><div class="imageblock" style=""><img src="images/gpu_cpu_dichotomy.svg" alt="gpu cpu dichotomy" width="100%"></div>
<aside class="notes"><div class="ulist"><ul><li><p>GPU: smallest unit of concurrency 32 (&gt;3000 cores)</p></li><li><p>CPU: smallest unit of concurrency 1 (10-20 cores)</p></li></ul></div></aside></section><section id="gpus_for_deep_learning_22"><h2>GPUs for Deep Learning 2/2</h2><div class="imageblock" style=""><img src="images/high_throughput_smx.svg" alt="high throughput smx" width="100%"></div>
<div class="paragraph"><p>Latency Hiding</p></div>
<aside class="notes"><div class="ulist"><ul><li><p>GPU: hides latency of memory access (larger bandwidth)</p></li><li><p>CPU: can hide latency to some degree only</p></li></ul></div></aside></section><section id="consequences_on_the_market"><h2>Consequences on the market</h2><div class="imageblock" style=""><img src="images/nvidia_stock.png" alt="nvidia stock"></div>
<div class="paragraph"><p>Nvidia&#8217;s stock pricing in the last years</p></div></section><section id="benchmarks"><h2>Benchmarks</h2><div class="imageblock" style=""><img src="images/directions.png" alt="directions"></div>
<aside class="notes"><div class="ulist"><ul><li><p>beginners typically don&#8217;t know where to go</p></li><li><p>which framework?</p></li><li><p>web is full of good advice</p></li></ul></div></aside></section><section id="deeprace"><h2><a href="https://github.com/psteinb/deeprace">deeprace</a></h2><div class="ulist"><ul><li><p>usable benchmark with clear <a href="https://semver.org">semver</a> support</p></li><li><p>model code is fixed</p></li><li><p><strong>ResNet</strong> (<a href="https://arxiv.org/pdf/1512.03385.pdf">v1</a>, <a href="https://arxiv.org/pdf/1603.05027.pdf">v2</a>), <a href="https://www.biorxiv.org/content/early/2018/01/23/236463.1">CARE Denoising network</a></p></li><li><p>Keras+TensorFlow or just TensorFlow</p></li><li><p>single and multi-gpu training (distributed planned)</p></li><li><p>data will be open-sourced once I find a sponsor</p></li></ul></div></section><section id="hardware"><h2>Hardware</h2><div class="ulist"><ul><li><p><strong>local cluster</strong>: <a href="https://doc.zih.tu-dresden.de/hpc-wiki/bin/view/Compendium/SystemTaurus">Taurus</a> at Technical University Dresden</p><div class="ulist"><ul><li><p>single GPU node:</p><div class="ulist"><ul><li><p>Intel Xeon E5-2680 v3 12c</p></li><li><p>64GB RAM</p></li><li><p>4x Nvidia Tesla K80 GPU</p></li></ul></div></li></ul></div></li><li><p>local servers (Nvidia Titan Xp, Nvidia Tesla P100)</p></li></ul></div></section><section id="using_resnet_on_cifar10"><h2>Using ResNet on CIFAR10</h2><div class="imageblock" style=""><img src="images/deeprace-full-single.svg" alt="deeprace full single"></div>
<aside class="notes"><div class="ulist"><ul><li><p>Resnet32v1 (and Resnet56v1) as sample models on CIFAR10 dataset</p></li><li><p>time-per-epoch higher for smaller batches (more host-device transfers, backprop more often)</p></li></ul></div></aside></section><section id="containers"><h2>Containers!</h2><div class="imageblock" style=""><img src="images/deeprace-full-vs-singularity.svg" alt="deeprace full vs singularity" width="100%"></div>
<div class="paragraph"><p><a href="https://www.sylabs.io/docs/">singularity</a> container = <a href="https://keras.io">Keras 2.1.5</a> + <a href="https://tensorflow.org">TensorFlow 1.3.0</a></p></div>
<aside class="notes"><div class="ulist"><ul><li><p>for setup and reproducibility</p></li><li><p>for the rest, use tf 1.7</p></li></ul></div></aside></section><section id="short_runs_only"><h2>Short runs only</h2><div class="imageblock" style=""><img src="images/deeprace-short-runtimes.svg" alt="deeprace short runtimes"></div>
<aside class="notes"><div class="ulist"><ul><li><p>as time per epoch is "flat" &#8594; limit to <code>n=15</code> epochs</p></li><li><p>multiple runs per measurements</p></li></ul></div></aside></section><section id="single_gpu_training"><h2>single-GPU training</h2><div class="imageblock" style=""><img src="images/deeprace-short-hw.png" alt="deeprace short hw"></div>
<aside class="notes"><div class="ulist"><ul><li><p>architecture difference Pascal (2016) and Kepler (2013/2014)</p></li><li><p>note: gaming GPUs</p></li></ul></div></aside></section><section id="cloud"><h2>cloud?</h2><div class="imageblock" style=""><img src="images/deeprace-short-runtimes-vs-cloud.svg" alt="deeprace short runtimes vs cloud"></div>
<div class="paragraph"><p>GCE, single K80 instance, 1vCPU, 6GB RAM, 10GB disk</p></div>
<aside class="notes"><div class="ulist"><ul><li><p>keras:2.1.5,tensorflow:1.7.0</p></li></ul></div></aside></section><section id="framework_differences"><h2>framework differences?</h2><div class="imageblock" style=""><img src="images/deeprace-frameworks.svg" alt="deeprace frameworks"></div>
<aside class="notes"><div class="ulist"><ul><li><p>Titan Xp</p></li></ul></div></aside></section></section>
<section><section id="wrapping_up"><h2>Wrapping up</h2></section><section id="deep_learning_2"><h2>Deep Learning?</h2><div class="ulist"><ul><li><p>for (unstructured) data like images/sequences/&#8230;&#8203;, neighborhood information crucial to extract information</p></li><li><p>deep convolutional neural networks have become state of the art in many domains</p></li><li><p>tentatively a lot of compute power is needed</p></li></ul></div></section><section id="relying_on_weights_can_be_dangerous"><h2>Relying on Weights can be dangerous</h2><div class="imageblock" style=""><img src="images/putin-on-horse.jpg" alt="putin on horse" width="100%"></div>
<div class="paragraph"><p>from <a href="https://twitter.com/genekogan/status/855032573327581185" class="bare">https://twitter.com/genekogan/status/855032573327581185</a></p></div>
<aside class="notes"><div class="ulist"><ul><li><p>DL as a black box?</p></li></ul></div></aside></section><section id="dl_as_a_black_box" data-background-iframe="https://lrpserver.hhi.fraunhofer.de/image-classification"><aside class="notes"><div class="ulist"><ul><li><p>Interpretability is a big topic</p></li></ul></div></aside></section></section>
<section><section id="trackml"><h2>TrackML?</h2></section><section id="board_leader_place_3"><h2>Board Leader Place 3</h2><div class="ulist"><ul><li class="fragment"><p>construct tracklets 2-3 layers(decision tree as code)</p></li><li class="fragment"><p>prolong tracks to the next layer &amp; find good match</p></li><li class="fragment"><p>recursively select best track based on metric</p></li></ul></div></section><section id="board_leader_place_2"><h2>Board Leader Place 2</h2><div class="ulist"><ul><li class="fragment"><p>neural network (4k-2k-2k-2k-1k neurons) to select pairs of hits that belong to one track</p></li><li class="fragment"><p>use seed hit, predict best pairing &amp; construct tracklet</p></li><li class="fragment"><p>extend tracklet until no hits left (track candidates)</p></li><li class="fragment"><p>recursively select best track from candidates in event</p></li></ul></div></section><section id="trackml_winner"><h2><a href="https://github.com/top-quarks/top-quarks">TrackML Winner</a></h2><div class="ulist"><ul><li class="fragment"><p>create pairs of hits in adjacent layers</p></li><li class="fragment"><p>prune pairs with logistic regression based on heuristics</p></li><li class="fragment"><p>create triples</p></li><li class="fragment"><p>prune triples with logistic regression based on heuristics</p></li><li class="fragment"><p>fit helix through triples to create candidate tracks</p></li><li class="fragment"><p>select best track (by random forest)</p></li></ul></div></section><section id="winning_strategy"><h2><a href="https://github.com/top-quarks/top-quarks">Winning Strategy</a></h2><div class="admonitionblock note"><table><tr><td class="icon"><i class="fa fa-info-circle" title="Note"></i></td><td class="content"><div class="paragraph"><p>Note that I have no knowledge of the field or generally used methods, and I spent little effort in trying to look at them. Therefore I don’t know what my novel findings are.</p></div></td></tr></table></div>
<div class="admonitionblock note"><table><tr><td class="icon"><i class="fa fa-info-circle" title="Note"></i></td><td class="content"><div class="paragraph"><p>I divided my algorithm into several steps, and created a scoring metric after each step, so that I could easily tell at which step I could earn the most score. I also made load / score function after each step for rapid debugging and tuning.</p></div></td></tr></table></div></section><section id="here_is_your_chance"><h2><a href="https://competitions.codalab.org/competitions/20112">Here is your chance!</a></h2><div class="paragraph"><p><span class="image"><img src="images/black-and-white-decision-doors-277017.jpg" alt="background"></span></p></div></section></section>
<section><section id="backup"><h2>Backup</h2></section><section id="trackml_tracking"><h2><a href="https://indico.cern.ch/event/587955/contributions/2937510/attachments/1683248/2705359/20180712-msmk-trackml-v3.pdf">TrackML Tracking</a></h2><div class="imageblock" style=""><img src="images/chep2018-mkiehn-p6.png" alt="chep2018 mkiehn p6"></div></section><section id="trackml_simulation"><h2><a href="https://indico.cern.ch/event/686555/contributions/2976579/attachments/1680748/2700965/ICHEP_TrackML_Jul052018.pdf">TrackML Simulation</a></h2><div class="imageblock" style=""><img src="images/ICHEP2018_TrackML_Jul052018_p12.png" alt="ICHEP2018 TrackML Jul052018 p12"></div></section></section></div></div><script src="reveal.js/lib/js/head.min.js"></script><script src="reveal.js/js/reveal.js"></script><script>// See https://github.com/hakimel/reveal.js#configuration for a full list of configuration options
Reveal.initialize({
  // Display controls in the bottom right corner
  controls: true,
  // Display a presentation progress bar
  progress: true,
  // Set a per-slide timing for speaker notes, null means none
  defaultTiming: null,
  // Display the page number of the current slide
  slideNumber: true,
  // Push each slide change to the browser history
  history: false,
  // Enable keyboard shortcuts for navigation
  keyboard: true,
  // Enable the slide overview mode
  overview: true,
  // Vertical centering of slides
  center: false,
  // Enables touch navigation on devices with touch input
  touch: true,
  // Loop the presentation
  loop: false,
  // Change the presentation direction to be RTL
  rtl: false,
  // Randomizes the order of slides each time the presentation loads
  shuffle: false,
  // Turns fragments on and off globally
  fragments: true,
  // Flags if the presentation is running in an embedded mode,
  // i.e. contained within a limited portion of the screen
  embedded: false,
  // Flags if we should show a help overlay when the questionmark
  // key is pressed
  help: true,
  // Flags if speaker notes should be visible to all viewers
  showNotes: false,
  // Global override for autolaying embedded media (video/audio/iframe)
  // - null: Media will only autoplay if data-autoplay is present
  // - true: All media will autoplay, regardless of individual setting
  // - false: No media will autoplay, regardless of individual setting
  autoPlayMedia: null,
  // Number of milliseconds between automatically proceeding to the
  // next slide, disabled when set to 0, this value can be overwritten
  // by using a data-autoslide attribute on your slides
  autoSlide: 0,
  // Stop auto-sliding after user input
  autoSlideStoppable: true,
  // Enable slide navigation via mouse wheel
  mouseWheel: false,
  // Hides the address bar on mobile devices
  hideAddressBar: true,
  // Opens links in an iframe preview overlay
  previewLinks: false,
  // Theme (e.g., beige, black, league, night, serif, simple, sky, solarized, white)
  // NOTE setting the theme in the config no longer works in reveal.js 3.x
  //theme: Reveal.getQueryHash().theme || 'black',
  // Transition style (e.g., none, fade, slide, convex, concave, zoom)
  transition: Reveal.getQueryHash().transition || 'slide',
  // Transition speed (e.g., default, fast, slow)
  transitionSpeed: 'default',
  // Transition style for full page slide backgrounds (e.g., none, fade, slide, convex, concave, zoom)
  backgroundTransition: 'fade',
  // Number of slides away from the current that are visible
  viewDistance: 3,
  // Parallax background image (e.g., "'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg'")
  parallaxBackgroundImage: '',
  // Parallax background size in CSS syntax (e.g., "2100px 900px")
  parallaxBackgroundSize: '',

  // The "normal" size of the presentation, aspect ratio will be preserved
  // when the presentation is scaled to fit different resolutions. Can be
  // specified using percentage units.
  width: 960,
  height: 700,

  // Factor of the display size that should remain empty around the content
  margin: 0.1,

  // Bounds for smallest/largest possible scale to apply to content
  minScale: 0.2,
  maxScale: 1.5,

  // Optional libraries used to extend on reveal.js
  dependencies: [
      { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
      { src: 'reveal.js/plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
      { src: 'reveal.js/plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
      
      { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
      { src: 'reveal.js/plugin/notes/notes.js', async: true }
  ]
});</script></body></html>